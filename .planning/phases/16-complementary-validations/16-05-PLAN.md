---
phase: 16-complementary-validations
plan: 05
type: execute
wave: 3
depends_on: ["16-03", "16-04"]
files_modified:
  - app/core/validation/orchestrator.py
  - app/tests/validation/test_orchestrator.py
  - app/tests/validation/test_benchmark.py
autonomous: true

must_haves:
  truths:
    - "ValidationOrchestrator runs ComplementaryValidator for thermography extractions"
    - "Benchmark test measures recall against ground truth dataset"
    - "CI fails if recall < 90% (quality gate)"
  artifacts:
    - path: "app/core/validation/orchestrator.py"
      provides: "Orchestrator with ComplementaryValidator integration"
      contains: "complementary_validator"
    - path: "app/tests/validation/test_benchmark.py"
      provides: "Benchmark test measuring recall on ground truth"
      contains: "test_recall"
  key_links:
    - from: "app/core/validation/orchestrator.py"
      to: "app/core/validation/complementary.py"
      via: "validator instantiation and calls"
      pattern: "self.complementary_validator"
    - from: "app/tests/validation/test_benchmark.py"
      to: "app/tests/fixtures/ground_truth.json"
      via: "pytest fixture loading"
      pattern: "ground_truth.json"
---

<objective>
Integrate ComplementaryValidator into orchestrator and create benchmark tests.

Purpose: ComplementaryValidator must be called by the orchestrator for thermography extractions. The benchmark tests measure recall against the ground truth dataset to ensure we're catching the errors we should catch. This provides regression testing to prevent future degradations.

Output: Updated orchestrator with complementary validation, benchmark test suite with recall measurement.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/16-complementary-validations/16-CONTEXT.md
@.planning/phases/16-complementary-validations/16-RESEARCH.md

# Orchestrator to update
@app/core/validation/orchestrator.py

# Ground truth from 16-02
# @app/tests/fixtures/ground_truth.json

# ComplementaryValidator from 16-03/16-04
# @app/core/validation/complementary.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integrate ComplementaryValidator into orchestrator</name>
  <files>app/core/validation/orchestrator.py</files>
  <action>
Update ValidationOrchestrator to include ComplementaryValidator:

1. Import ComplementaryValidator:
```python
from app.core.validation.complementary import ComplementaryValidator
```

2. Add to __init__:
```python
self.complementary_validator = ComplementaryValidator(self.config)
```

3. Add complementary validation step in validate() method for thermography:
```python
def validate(
    self,
    extraction: BaseExtractionResult,
    certificate_ocr: CertificateOCRResult | None = None,
    hygrometer_ocr: HygrometerOCRResult | None = None,
    report_comments: str | None = None,
    expected_phases: list[str] | None = None,
) -> ValidationResult:
    """Validate extraction using appropriate validators.

    Args:
        extraction: The extraction result to validate.
        certificate_ocr: Optional OCR result from calibration certificate.
        hygrometer_ocr: Optional OCR result from thermo-hygrometer photo.
        report_comments: Optional comments section text for SPEC compliance.
        expected_phases: Optional list of expected phase identifiers.

    Returns:
        ValidationResult with all findings combined.
    """
    all_findings: list[Finding] = []
    # ... existing code ...

    # After test-type specific validation, calibration, and cross-field:

    # 4. Complementary validations (for thermography only)
    if isinstance(extraction, ThermographyExtractionResult):
        comp_result = self.complementary_validator.validate(
            extraction,
            certificate_ocr=certificate_ocr,
            hygrometer_ocr=hygrometer_ocr,
            report_comments=report_comments,
            expected_phases=expected_phases,
        )
        all_findings.extend(comp_result.findings)

    return ValidationResult(...)
```

Also add the necessary imports for OCR types at the top of the file.

Note: The orchestrator's validate() signature now accepts optional OCR data. This maintains backward compatibility - callers that don't pass OCR data still work.
  </action>
  <verify>
```bash
cd /home/xande && python -c "
from app.core.validation.orchestrator import ValidationOrchestrator
o = ValidationOrchestrator()
assert hasattr(o, 'complementary_validator')
print('ComplementaryValidator integrated into orchestrator')
"
```
  </verify>
  <done>ValidationOrchestrator includes ComplementaryValidator and calls it for thermography extractions</done>
</task>

<task type="auto">
  <name>Task 2: Update orchestrator tests</name>
  <files>app/tests/validation/test_orchestrator.py</files>
  <action>
Add tests to verify ComplementaryValidator integration:

```python
# Add to test_orchestrator.py

from app.core.extraction.ocr import CertificateOCRResult
from app.core.extraction.schemas import CalibrationInfo


class TestComplementaryIntegration:
    """Test ComplementaryValidator integration in orchestrator."""

    def test_orchestrator_runs_complementary_validator(self):
        """Verify orchestrator runs complementary validation for thermography."""
        orchestrator = ValidationOrchestrator()

        # Create extraction with expired calibration
        extraction = ThermographyExtractionResult(
            equipment=EquipmentInfo(
                equipment_tag=FieldConfidence(value="QD-01", confidence=0.9),
                equipment_type=FieldConfidence(value="panel", confidence=0.9),
            ),
            calibration=CalibrationInfo(
                expiration_date=FieldConfidence(value="2024-01-01", confidence=0.9),
            ),
            test_conditions=ThermographyTestConditions(
                inspection_date=FieldConfidence(value="2024-06-15", confidence=0.9),
            ),
            thermal_data=ThermalImageData(),
            hotspots=[],
            overall_confidence=0.9,
        )

        result = orchestrator.validate(extraction)

        # Should have COMP-001 finding from complementary validator
        comp_findings = [f for f in result.findings if f.rule_id.startswith("COMP-")]
        assert len(comp_findings) >= 1
        assert any(f.rule_id == "COMP-001" for f in comp_findings)

    def test_orchestrator_passes_ocr_to_complementary(self):
        """Verify orchestrator passes OCR data to complementary validator."""
        orchestrator = ValidationOrchestrator()

        extraction = ThermographyExtractionResult(
            equipment=EquipmentInfo(
                equipment_tag=FieldConfidence(value="QD-01", confidence=0.9),
                equipment_type=FieldConfidence(value="panel", confidence=0.9),
            ),
            test_conditions=ThermographyTestConditions(
                inspection_date=FieldConfidence(value="2024-06-15", confidence=0.9),
                camera_serial=FieldConfidence(value="ABC123", confidence=0.9),
            ),
            thermal_data=ThermalImageData(),
            hotspots=[],
            overall_confidence=0.9,
        )

        # Pass OCR with mismatching serial
        certificate_ocr = CertificateOCRResult(
            serial_number=FieldConfidence(value="XYZ789", confidence=0.95),
        )

        result = orchestrator.validate(extraction, certificate_ocr=certificate_ocr)

        # Should have COMP-002 finding for serial mismatch
        comp_findings = [f for f in result.findings if f.rule_id == "COMP-002"]
        assert len(comp_findings) == 1

    def test_orchestrator_without_ocr_still_works(self):
        """Verify orchestrator works when no OCR data provided."""
        orchestrator = ValidationOrchestrator()

        extraction = ThermographyExtractionResult(
            equipment=EquipmentInfo(
                equipment_tag=FieldConfidence(value="QD-01", confidence=0.9),
                equipment_type=FieldConfidence(value="panel", confidence=0.9),
            ),
            test_conditions=ThermographyTestConditions(
                inspection_date=FieldConfidence(value="2024-06-15", confidence=0.9),
            ),
            thermal_data=ThermalImageData(),
            hotspots=[],
            overall_confidence=0.9,
        )

        # No OCR data - should still validate without errors
        result = orchestrator.validate(extraction)

        assert result is not None
        assert isinstance(result.findings, list)
```
  </action>
  <verify>
```bash
cd /home/xande && python -m pytest app/tests/validation/test_orchestrator.py -v -k "complementary or Complementary"
```
  </verify>
  <done>Orchestrator tests verify ComplementaryValidator integration</done>
</task>

<task type="auto">
  <name>Task 3: Create benchmark test suite</name>
  <files>app/tests/validation/test_benchmark.py</files>
  <action>
Create benchmark test suite that measures recall against ground truth:

```python
"""Benchmark tests for complementary validation recall.

These tests measure detection accuracy against a ground truth dataset
of manually reviewed reports. The benchmark serves as a regression test
to prevent future degradations in detection accuracy.

Per CONTEXT.md:
- Target: >95% recall on the 10-report ground truth dataset
- Quality gate: If recall < 90%, fail the build
"""

import json
from pathlib import Path

import pytest

# Ground truth dataset location
GROUND_TRUTH_PATH = Path("app/tests/fixtures/ground_truth.json")


@pytest.fixture
def ground_truth():
    """Load ground truth dataset."""
    if not GROUND_TRUTH_PATH.exists():
        pytest.skip("Ground truth dataset not found")
    with open(GROUND_TRUTH_PATH) as f:
        return json.load(f)


class TestBenchmarkStructure:
    """Tests for ground truth dataset structure."""

    def test_ground_truth_exists(self, ground_truth):
        """Ground truth file exists and is valid JSON."""
        assert ground_truth is not None
        assert "version" in ground_truth
        assert "reports" in ground_truth

    def test_ground_truth_has_reports(self, ground_truth):
        """Ground truth has expected number of reports."""
        assert len(ground_truth["reports"]) == 10

    def test_ground_truth_balanced(self, ground_truth):
        """Ground truth has balanced rejected/approved pairs."""
        rejected = [r for r in ground_truth["reports"] if r["expected_verdict"] == "rejected"]
        approved = [r for r in ground_truth["reports"] if r["expected_verdict"] == "approved"]
        assert len(rejected) == 5
        assert len(approved) == 5

    def test_finding_codes_defined(self, ground_truth):
        """All expected findings have code definitions."""
        defined_codes = set(ground_truth["finding_codes"].keys())
        for report in ground_truth["reports"]:
            for finding in report["expected_findings"]:
                assert finding["code"] in defined_codes, f"Undefined code: {finding['code']}"


class TestFindingCodeMapping:
    """Tests for finding code to rule_id mapping."""

    def test_all_codes_have_rule_ids(self, ground_truth):
        """Each finding code maps to a rule_id."""
        for code, definition in ground_truth["finding_codes"].items():
            assert "rule_id" in definition, f"{code} missing rule_id"
            assert definition["rule_id"].startswith("COMP-"), f"{code} rule_id should start with COMP-"

    def test_rule_id_mapping(self, ground_truth):
        """Finding codes map to expected rule IDs."""
        expected_mapping = {
            "CALIBRATION_EXPIRED": "COMP-001",
            "SERIAL_MISMATCH": "COMP-002",
            "VALUE_MISMATCH": "COMP-003",
            "PHOTO_MISSING": "COMP-004",
            "SPEC_NON_COMPLIANCE": "COMP-005",
            "SERIAL_ILLEGIBLE": "COMP-006",
        }
        for code, expected_rule in expected_mapping.items():
            if code in ground_truth["finding_codes"]:
                actual_rule = ground_truth["finding_codes"][code]["rule_id"]
                assert actual_rule == expected_rule, f"{code}: expected {expected_rule}, got {actual_rule}"


class TestRecallMetrics:
    """Tests for recall calculation helpers.

    Note: Actual recall tests require PDF processing which needs
    the full extraction pipeline. These tests verify the metric
    calculation infrastructure.
    """

    def test_recall_calculation(self):
        """Test recall calculation formula."""
        # recall = true_positives / (true_positives + false_negatives)
        # If we expect 10 findings and detect 9, recall = 9/10 = 90%

        def calculate_recall(expected: int, detected: int) -> float:
            if expected == 0:
                return 1.0  # No expected findings = 100% recall
            return detected / expected

        assert calculate_recall(10, 10) == 1.0
        assert calculate_recall(10, 9) == 0.9
        assert calculate_recall(10, 5) == 0.5
        assert calculate_recall(0, 0) == 1.0

    def test_recall_threshold(self, ground_truth):
        """Verify target recall is documented."""
        assert "target_recall" in ground_truth
        assert ground_truth["target_recall"] >= 0.90, "Target recall should be >= 90%"


# Placeholder for integration tests that require PDF processing
# These will be implemented when actual report PDFs are available
class TestRecallIntegration:
    """Integration tests for recall on ground truth reports.

    These tests require actual PDF reports in the fixtures directory.
    They process each report through the full pipeline and compare
    detected findings against expected findings.
    """

    @pytest.mark.skip(reason="Requires PDF reports in fixtures - manual setup")
    def test_individual_report_recall(self, ground_truth):
        """Each report should detect all expected findings."""
        # TODO: Implement when PDFs are available
        # For each report:
        #   1. Load PDF
        #   2. Run extraction pipeline
        #   3. Run validation with OCR
        #   4. Compare findings against expected
        pass

    @pytest.mark.skip(reason="Requires PDF reports in fixtures - manual setup")
    def test_overall_recall_threshold(self, ground_truth):
        """Overall recall should meet target threshold."""
        # TODO: Implement when PDFs are available
        # Calculate aggregate recall across all reports
        # Assert recall >= ground_truth["target_recall"]
        pass
```

Note: The integration tests are marked as skip because they require actual PDF reports. The test infrastructure is in place for when reports are added.
  </action>
  <verify>
```bash
cd /home/xande && python -m pytest app/tests/validation/test_benchmark.py -v
```
  </verify>
  <done>Benchmark test suite exists with structure tests and recall calculation helpers</done>
</task>

</tasks>

<verification>
```bash
# Orchestrator integration works
cd /home/xande && python -c "
from app.core.validation.orchestrator import ValidationOrchestrator, validate_extraction
from app.core.extraction.thermography import ThermographyExtractionResult, ThermalImageData, ThermographyTestConditions
from app.core.extraction.schemas import EquipmentInfo, FieldConfidence, CalibrationInfo

o = ValidationOrchestrator()
extraction = ThermographyExtractionResult(
    equipment=EquipmentInfo(
        equipment_tag=FieldConfidence(value='QD-01', confidence=0.9),
        equipment_type=FieldConfidence(value='panel', confidence=0.9),
    ),
    calibration=CalibrationInfo(
        expiration_date=FieldConfidence(value='2024-01-01', confidence=0.9),
    ),
    test_conditions=ThermographyTestConditions(
        inspection_date=FieldConfidence(value='2024-06-15', confidence=0.9),
    ),
    thermal_data=ThermalImageData(),
    hotspots=[],
    overall_confidence=0.9,
)
result = o.validate(extraction)
comp_findings = [f for f in result.findings if f.rule_id.startswith('COMP-')]
print(f'Complementary findings: {[f.rule_id for f in comp_findings]}')
assert any(f.rule_id == 'COMP-001' for f in comp_findings), 'Expected COMP-001 finding'
print('Orchestrator integration verified')
"

# All tests pass
cd /home/xande && python -m pytest app/tests/validation/test_orchestrator.py app/tests/validation/test_benchmark.py -v
```
</verification>

<success_criteria>
- ValidationOrchestrator has complementary_validator attribute
- Orchestrator's validate() accepts optional OCR and comments parameters
- Orchestrator runs ComplementaryValidator for thermography extractions
- Benchmark tests verify ground truth structure and finding code mapping
- Recall calculation helpers are tested
- Integration test placeholders exist for when PDFs are available
- All existing tests still pass
</success_criteria>

<output>
After completion, create `.planning/phases/16-complementary-validations/16-05-SUMMARY.md`
</output>
