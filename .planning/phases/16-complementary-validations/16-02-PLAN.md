---
phase: 16-complementary-validations
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - app/tests/fixtures/ground_truth.json
  - app/tests/fixtures/reports/.gitkeep
  - app/core/validation/config.py
autonomous: true

must_haves:
  truths:
    - "Ground truth dataset defines expected findings for 10 reports"
    - "Each rejected report has corresponding expected finding codes"
    - "Benchmark config thresholds are configurable"
  artifacts:
    - path: "app/tests/fixtures/ground_truth.json"
      provides: "Ground truth dataset with expected findings per report"
      contains: "expected_findings"
    - path: "app/core/validation/config.py"
      provides: "ComplementaryConfig with configurable thresholds"
      contains: "ComplementaryConfig"
  key_links:
    - from: "app/tests/fixtures/ground_truth.json"
      to: "app/tests/validation/test_benchmark.py"
      via: "pytest fixture loading"
      pattern: "ground_truth.json"
---

<objective>
Create ground truth dataset and complementary validation configuration.

Purpose: The benchmark requires a ground truth dataset defining expected findings for each report. This enables recall measurement (are we catching the errors we should catch?). Additionally, complementary validators need configurable thresholds (e.g., temperature match tolerance, serial confidence threshold).

Output: Ground truth JSON fixture with 10-report dataset, and ComplementaryConfig class added to validation config.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/16-complementary-validations/16-CONTEXT.md
@.planning/phases/16-complementary-validations/16-RESEARCH.md

# Existing config patterns
@app/core/validation/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ground truth dataset structure</name>
  <files>app/tests/fixtures/ground_truth.json, app/tests/fixtures/reports/.gitkeep</files>
  <action>
Create the ground truth dataset JSON file with the following structure:

```json
{
  "version": "1.0",
  "description": "Ground truth dataset for complementary validation benchmark",
  "target_recall": 0.95,
  "reports": [
    {
      "report_id": "rejected_001",
      "file_path": "app/tests/fixtures/reports/rejected_001.pdf",
      "expected_verdict": "rejected",
      "expected_findings": [
        {
          "code": "CALIBRATION_EXPIRED",
          "blocker": true,
          "description": "Calibration certificate expired before inspection date"
        }
      ],
      "pair_with": "approved_001",
      "notes": "Calibration expiration test case"
    },
    {
      "report_id": "approved_001",
      "file_path": "app/tests/fixtures/reports/approved_001.pdf",
      "expected_verdict": "approved",
      "expected_findings": [],
      "pair_with": "rejected_001",
      "notes": "Corrected version of rejected_001 - valid calibration"
    },
    {
      "report_id": "rejected_002",
      "file_path": "app/tests/fixtures/reports/rejected_002.pdf",
      "expected_verdict": "rejected",
      "expected_findings": [
        {
          "code": "SERIAL_MISMATCH",
          "blocker": true,
          "description": "Camera serial in report differs from certificate photo"
        }
      ],
      "pair_with": "approved_002",
      "notes": "Serial number mismatch test case"
    },
    {
      "report_id": "approved_002",
      "file_path": "app/tests/fixtures/reports/approved_002.pdf",
      "expected_verdict": "approved",
      "expected_findings": [],
      "pair_with": "rejected_002",
      "notes": "Corrected version - matching serial numbers"
    },
    {
      "report_id": "rejected_003",
      "file_path": "app/tests/fixtures/reports/rejected_003.pdf",
      "expected_verdict": "rejected",
      "expected_findings": [
        {
          "code": "VALUE_MISMATCH",
          "blocker": true,
          "description": "Reflected temp in report differs from hygrometer reading"
        }
      ],
      "pair_with": "approved_003",
      "notes": "Temperature value mismatch test case"
    },
    {
      "report_id": "approved_003",
      "file_path": "app/tests/fixtures/reports/approved_003.pdf",
      "expected_verdict": "approved",
      "expected_findings": [],
      "pair_with": "rejected_003",
      "notes": "Corrected version - matching temperature values"
    },
    {
      "report_id": "rejected_004",
      "file_path": "app/tests/fixtures/reports/rejected_004.pdf",
      "expected_verdict": "rejected",
      "expected_findings": [
        {
          "code": "PHOTO_MISSING",
          "blocker": true,
          "description": "Phase B thermal image missing"
        }
      ],
      "pair_with": "approved_004",
      "notes": "Missing photo test case"
    },
    {
      "report_id": "approved_004",
      "file_path": "app/tests/fixtures/reports/approved_004.pdf",
      "expected_verdict": "approved",
      "expected_findings": [],
      "pair_with": "rejected_004",
      "notes": "Corrected version - all phase photos present"
    },
    {
      "report_id": "rejected_005",
      "file_path": "app/tests/fixtures/reports/rejected_005.pdf",
      "expected_verdict": "rejected",
      "expected_findings": [
        {
          "code": "SPEC_NON_COMPLIANCE",
          "blocker": true,
          "description": "Delta-T > 10C without required comments"
        }
      ],
      "pair_with": "approved_005",
      "notes": "SPEC compliance test case"
    },
    {
      "report_id": "approved_005",
      "file_path": "app/tests/fixtures/reports/approved_005.pdf",
      "expected_verdict": "approved",
      "expected_findings": [],
      "pair_with": "rejected_005",
      "notes": "Corrected version - proper comments for high delta-T"
    }
  ],
  "finding_codes": {
    "CALIBRATION_EXPIRED": {
      "rule_id": "COMP-001",
      "description": "Calibration certificate expired before measurement date",
      "severity": "critical"
    },
    "SERIAL_MISMATCH": {
      "rule_id": "COMP-002",
      "description": "Camera serial number mismatch between report and certificate",
      "severity": "critical"
    },
    "VALUE_MISMATCH": {
      "rule_id": "COMP-003",
      "description": "Reflected temperature mismatch between report and hygrometer",
      "severity": "critical"
    },
    "PHOTO_MISSING": {
      "rule_id": "COMP-004",
      "description": "Required thermal image missing for one or more phases",
      "severity": "critical"
    },
    "SPEC_NON_COMPLIANCE": {
      "rule_id": "COMP-005",
      "description": "Delta-T exceeds 10C without required documentation",
      "severity": "critical"
    },
    "SERIAL_ILLEGIBLE": {
      "rule_id": "COMP-006",
      "description": "Serial number could not be read with sufficient confidence",
      "severity": "major"
    }
  }
}
```

Create directories if they do not exist:
- `app/tests/fixtures/` (may exist)
- `app/tests/fixtures/reports/` (create with .gitkeep for future PDFs)

Note: Actual PDF reports will be added manually later. The fixture structure is what we need now.
  </action>
  <verify>
```bash
cd /home/xande && python -c "
import json
with open('app/tests/fixtures/ground_truth.json') as f:
    data = json.load(f)
assert data['version'] == '1.0'
assert len(data['reports']) == 10
assert data['target_recall'] == 0.95
print(f'Ground truth: {len(data[\"reports\"])} reports, {len(data[\"finding_codes\"])} finding codes')
"
```
  </verify>
  <done>Ground truth JSON exists with 10 reports (5 rejected/approved pairs) and finding code definitions</done>
</task>

<task type="auto">
  <name>Task 2: Add ComplementaryConfig to validation config</name>
  <files>app/core/validation/config.py</files>
  <action>
Add ComplementaryConfig class to validation config with configurable thresholds for complementary validators:

```python
class ComplementaryConfig(BaseModel):
    """Complementary validation settings.

    Thresholds for cross-validation rules that complement
    test-type specific validators.
    """

    # Serial number OCR confidence threshold
    # Below this = SERIAL_ILLEGIBLE (review flag, not blocker)
    serial_confidence_threshold: float = 0.7

    # Temperature match tolerance in Celsius
    # Reflected temp vs hygrometer reading can differ by this amount
    temp_match_tolerance: float = 2.0

    # SPEC compliance threshold (Microsoft standard)
    spec_delta_t_threshold: float = 10.0

    # Required keywords in COMMENTS section when delta > threshold
    spec_required_keywords: list[str] = Field(
        default_factory=lambda: [
            "terminals", "insulators", "torque", "conductors",
            "terminais", "isoladores", "torque", "condutores"  # Portuguese
        ]
    )

    # Standard reference for complementary validations
    standard_reference: str = "Microsoft SPEC 26 05 00"
```

Add `complementary: ComplementaryConfig` field to the main `ValidationConfig` class.

Update `get_config_for_standard()` to include complementary config (same for both NETA and Microsoft - these are cross-validation rules).
  </action>
  <verify>
```bash
cd /home/xande && python -c "
from app.core.validation.config import ValidationConfig, ComplementaryConfig, get_validation_config
config = get_validation_config()
assert hasattr(config, 'complementary')
assert config.complementary.serial_confidence_threshold == 0.7
assert config.complementary.temp_match_tolerance == 2.0
assert config.complementary.spec_delta_t_threshold == 10.0
print('ComplementaryConfig OK:', config.complementary)
"
```
  </verify>
  <done>ComplementaryConfig added to ValidationConfig with configurable thresholds for all 5 validators</done>
</task>

</tasks>

<verification>
```bash
# Ground truth structure valid
cd /home/xande && python -c "
import json
with open('app/tests/fixtures/ground_truth.json') as f:
    gt = json.load(f)

# Validate structure
for report in gt['reports']:
    assert 'report_id' in report
    assert 'expected_verdict' in report
    assert 'expected_findings' in report

rejected = [r for r in gt['reports'] if r['expected_verdict'] == 'rejected']
approved = [r for r in gt['reports'] if r['expected_verdict'] == 'approved']
assert len(rejected) == 5, 'Expected 5 rejected reports'
assert len(approved) == 5, 'Expected 5 approved reports'
print('Ground truth validation passed')
"

# Config imports work
cd /home/xande && python -c "
from app.core.validation.config import get_validation_config
config = get_validation_config()
print(f'Complementary config: {config.complementary}')
"
```
</verification>

<success_criteria>
- Ground truth JSON with 10 reports (5 rejected/5 approved pairs)
- Each report has expected_verdict and expected_findings
- Finding codes documented with rule_id, description, severity
- ComplementaryConfig added to ValidationConfig
- Configurable thresholds: serial_confidence, temp_tolerance, spec_delta_t
- All existing tests still pass
</success_criteria>

<output>
After completion, create `.planning/phases/16-complementary-validations/16-02-SUMMARY.md`
</output>
