---
phase: 02-extraction-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [app/api/upload.py, app/services/__init__.py, app/services/storage.py, app/schemas/upload.py]
autonomous: true

must_haves:
  truths:
    - "User can upload PDF files up to 50MB via POST /api/upload"
    - "User can upload image files (PNG, JPG, TIFF) via POST /api/upload"
    - "Uploaded files are stored in configurable storage path"
    - "Upload creates a Task record with QUEUED status"
  artifacts:
    - path: "app/api/upload.py"
      provides: "File upload endpoint"
      contains: "@router.post"
    - path: "app/services/storage.py"
      provides: "File storage service"
      exports: ["save_file", "get_file_path"]
    - path: "app/schemas/upload.py"
      provides: "Upload request/response schemas"
      min_lines: 20
  key_links:
    - from: "app/api/upload.py"
      to: "app/services/storage.py"
      via: "calls save_file"
      pattern: "save_file"
    - from: "app/api/upload.py"
      to: "app/db/models/task.py"
      via: "creates Task record"
      pattern: "Task"
---

<objective>
Create file upload API endpoint with storage service for commissioning report PDFs and images.

Purpose: Enable users to submit documents for analysis. Files are stored and queued for background extraction processing.

Output: POST /api/upload endpoint that accepts PDF/images, stores them, and creates Task records.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
@.planning/phases/01-foundation/01-02-SUMMARY.md
@.planning/phases/01-foundation/01-05-SUMMARY.md

Requirements this plan addresses:
- UPLD-01: User can upload PDF files up to 50MB
- UPLD-02: User can upload image files (PNG, JPG, TIFF) for thermal images
- UPLD-03: System queues uploaded documents for background processing
- UPLD-05: System stores original documents for reprocessing capability
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create upload schemas</name>
  <files>app/schemas/upload.py</files>
  <action>
Create Pydantic schemas for file upload.

app/schemas/upload.py:
- UploadResponse: task_id (UUID), filename (str), file_size (int), status (TaskStatus), created_at (datetime)
- UploadError: error_code (str), detail (str)
- AllowedFileTypes: Enum with PDF, PNG, JPG, JPEG, TIFF values
- MAX_FILE_SIZE: constant = 50 * 1024 * 1024 (50MB)

Add validation for:
- File extension must be in AllowedFileTypes
- File size must be <= MAX_FILE_SIZE

Update app/schemas/__init__.py to export new schemas.
  </action>
  <verify>python3 -c "from app.schemas.upload import UploadResponse, MAX_FILE_SIZE"</verify>
  <done>Upload schemas defined with validation constants</done>
</task>

<task type="auto">
  <name>Task 2: Create storage service</name>
  <files>app/services/__init__.py, app/services/storage.py</files>
  <action>
Create file storage service.

app/services/storage.py:
- UPLOAD_DIR from settings (default: "./uploads")
- Ensure upload directory exists on module load

Functions:
- get_file_path(task_id: UUID, filename: str) -> Path
  - Returns: UPLOAD_DIR / str(task_id) / filename

- async save_file(task_id: UUID, file: UploadFile) -> tuple[Path, int]
  - Create task subdirectory
  - Stream file to disk in chunks (avoid loading 50MB in memory)
  - Return (file_path, file_size)

- async get_file(task_id: UUID, filename: str) -> Path | None
  - Return file path if exists, None otherwise

- delete_task_files(task_id: UUID) -> None
  - Remove task directory and all files

Use aiofiles for async file operations.

app/services/__init__.py:
- Export storage functions
  </action>
  <verify>python3 -c "from app.services.storage import save_file, get_file_path"</verify>
  <done>Storage service handles file save/retrieve with streaming</done>
</task>

<task type="auto">
  <name>Task 3: Create upload API endpoint</name>
  <files>app/api/upload.py, app/main.py</files>
  <action>
Create file upload endpoint.

app/api/upload.py:
- router = APIRouter(prefix="/api", tags=["upload"])

POST /api/upload:
- Accept multipart/form-data with file field
- Validate file extension against AllowedFileTypes
- Validate file size (reject if > 50MB with 413 status)
- Create Task record with:
  - status = QUEUED
  - original_filename = file.filename
  - file_size = actual size after save
  - user_id = from authenticated user (placeholder for now, can be None)
- Save file using storage service
- Update Task with file_path
- Enqueue processing job (call enqueue_task from worker)
- Return UploadResponse

Handle errors:
- 400 for invalid file type
- 413 for file too large
- 500 for storage errors

Update app/main.py to include upload router.
  </action>
  <verify>curl -X POST http://localhost:8000/api/upload -F "file=@test.pdf" (after starting server)</verify>
  <done>Upload endpoint accepts files, stores them, creates Task, queues job</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] POST /api/upload accepts PDF files
- [ ] POST /api/upload accepts image files (PNG, JPG, TIFF)
- [ ] Files larger than 50MB are rejected with 413
- [ ] Invalid file types are rejected with 400
- [ ] Task record is created with QUEUED status
- [ ] File is stored in uploads/{task_id}/ directory
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- No security vulnerabilities (path traversal, etc.)
- Streaming upload handles large files efficiently
</success_criteria>

<output>
After completion, create `.planning/phases/02-extraction-pipeline/02-01-SUMMARY.md`
</output>
