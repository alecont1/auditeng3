---
phase: 02-extraction-pipeline
plan: 06
type: execute
wave: 3
depends_on: ["02-01", "02-03", "02-04", "02-05"]
files_modified: [app/worker/extraction.py, app/services/extraction.py, app/api/tasks.py]
autonomous: true

must_haves:
  truths:
    - "Background worker processes uploaded documents"
    - "Multi-page PDFs are processed page by page"
    - "Processing status is trackable via API"
    - "Low-confidence extractions are flagged"
  artifacts:
    - path: "app/worker/extraction.py"
      provides: "Extraction background task"
      contains: "@dramatiq.actor"
    - path: "app/services/extraction.py"
      provides: "Extraction orchestration service"
      exports: ["process_document", "detect_test_type"]
    - path: "app/api/tasks.py"
      provides: "Task status API endpoint"
      contains: "@router.get"
  key_links:
    - from: "app/worker/extraction.py"
      to: "app/services/extraction.py"
      via: "calls process_document"
      pattern: "process_document"
    - from: "app/api/tasks.py"
      to: "app/db/models/task.py"
      via: "queries Task status"
      pattern: "Task"
---

<objective>
Create extraction orchestration that processes uploaded documents, detects test type, routes to appropriate extractor, and tracks status.

Purpose: Wire together upload → extraction → storage pipeline with multi-page PDF support and status tracking.

Output: Complete extraction pipeline from file upload to stored results with status API.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-extraction-pipeline/02-01-SUMMARY.md
@.planning/phases/02-extraction-pipeline/02-02-SUMMARY.md
@.planning/phases/02-extraction-pipeline/02-03-SUMMARY.md
@.planning/phases/02-extraction-pipeline/02-04-SUMMARY.md
@.planning/phases/02-extraction-pipeline/02-05-SUMMARY.md

Requirements this plan addresses:
- UPLD-04: User can view processing status of uploaded documents
- UPLD-06: System handles multi-page PDF documents with page-level processing
- EXTR-07: System assigns confidence score (0-1) to each extracted field
- EXTR-08: System flags low-confidence extractions for human review
- EXTR-09: System retries extraction on validation failure (max 3 attempts)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create extraction orchestration service</name>
  <files>app/services/extraction.py</files>
  <action>
Create extraction service that orchestrates the pipeline.

app/services/extraction.py:
- Import all extractors (GroundingExtractor, MeggerExtractor, ThermographyExtractor)
- Import from app.core.extraction.schemas import BaseExtractionResult

async def detect_test_type(content: str) -> str:
  """
  Detect test type from document content using Claude.

  Returns: 'grounding', 'megger', 'thermography', or 'unknown'
  """
  # Use Claude to classify the document
  # Look for keywords: "insulation resistance", "megger", "IR test" → megger
  # "ground", "earth", "resistance to ground" → grounding
  # "thermal", "infrared", "temperature" → thermography
  pass

def get_extractor(test_type: str) -> BaseExtractor:
  """Get extractor instance for test type."""
  extractors = {
      "grounding": GroundingExtractor(),
      "megger": MeggerExtractor(),
      "thermography": ThermographyExtractor(),
  }
  return extractors.get(test_type)

async def extract_pdf_text(file_path: Path) -> list[tuple[int, str]]:
  """
  Extract text from PDF, page by page.

  Returns: List of (page_number, text_content)
  """
  import pymupdf  # PyMuPDF

  pages = []
  doc = pymupdf.open(file_path)
  for page_num, page in enumerate(doc, 1):
      text = page.get_text()
      pages.append((page_num, text))
  doc.close()
  return pages

async def extract_pdf_images(file_path: Path) -> list[tuple[int, bytes]]:
  """
  Extract images from PDF for thermal analysis.

  Returns: List of (page_number, image_bytes)
  """
  import pymupdf

  images = []
  doc = pymupdf.open(file_path)
  for page_num, page in enumerate(doc, 1):
      image_list = page.get_images()
      for img in image_list:
          xref = img[0]
          base_image = doc.extract_image(xref)
          images.append((page_num, base_image["image"]))
  doc.close()
  return images

async def process_document(
  task_id: UUID,
  file_path: Path,
  test_type: str | None = None,
) -> BaseExtractionResult:
  """
  Process a document through the extraction pipeline.

  Args:
      task_id: Task ID for tracking
      file_path: Path to uploaded file
      test_type: Optional test type (auto-detected if not provided)

  Returns:
      Extraction result from appropriate extractor
  """
  # 1. Determine file type
  suffix = file_path.suffix.lower()
  is_image = suffix in ['.png', '.jpg', '.jpeg', '.tiff']

  # 2. Extract content
  if is_image:
      # Direct image - likely thermography
      with open(file_path, 'rb') as f:
          image_data = f.read()
      content = [image_data]
      test_type = test_type or "thermography"
  else:
      # PDF - extract text and detect type
      pages = await extract_pdf_text(file_path)
      all_text = "\n".join(text for _, text in pages)

      if not test_type:
          test_type = await detect_test_type(all_text)

      if test_type == "thermography":
          # Extract images from PDF
          content = await extract_pdf_images(file_path)
      else:
          content = all_text

  # 3. Get appropriate extractor
  extractor = get_extractor(test_type)
  if not extractor:
      raise ValueError(f"Unknown test type: {test_type}")

  # 4. Run extraction
  if test_type == "thermography" and isinstance(content, list):
      # Special handling for thermal images
      result = await extractor.extract_from_images(
          images=[img for _, img in content],
          page_numbers=[pg for pg, _ in content],
      )
  else:
      result = await extractor.extract(content=content)

  return result

Update app/services/__init__.py to export these functions.
  </action>
  <verify>python3 -c "from app.services.extraction import process_document, detect_test_type"</verify>
  <done>Extraction orchestration with test type detection and PDF handling</done>
</task>

<task type="auto">
  <name>Task 2: Create extraction worker task</name>
  <files>app/worker/extraction.py</files>
  <action>
Create Dramatiq actor for background extraction.

app/worker/extraction.py:
- Import dramatiq
- Import from app.services.extraction import process_document
- Import from app.db.session import async_session_factory
- Import from app.db.models import Task, Analysis
- Import from app.schemas.enums import TaskStatus

@dramatiq.actor(
  max_retries=3,
  min_backoff=1000,
  max_backoff=300000,
  queue_name="default",
)
def process_document_task(task_id: str):
  """
  Background task to process uploaded document.

  Args:
      task_id: UUID string of the task to process
  """
  import asyncio
  asyncio.run(_process_document_async(task_id))

async def _process_document_async(task_id: str):
  """Async implementation of document processing."""
  from uuid import UUID
  from pathlib import Path
  from app.worker.status import set_job_status, JobStatus

  task_uuid = UUID(task_id)

  async with async_session_factory() as session:
      # 1. Get task and update status
      task = await session.get(Task, task_uuid)
      if not task:
          set_job_status(task_id, JobStatus.FAILED, error="Task not found")
          return

      task.status = TaskStatus.PROCESSING.value
      await session.commit()
      set_job_status(task_id, JobStatus.PROCESSING)

      try:
          # 2. Run extraction
          file_path = Path(task.file_path)
          result = await process_document(task_uuid, file_path)

          # 3. Create Analysis record
          analysis = Analysis(
              task_id=task_uuid,
              equipment_type=result.equipment.equipment_type.value if hasattr(result, 'equipment') else None,
              test_type=result.metadata.test_type if hasattr(result.metadata, 'test_type') else None,
              equipment_tag=result.equipment.equipment_tag.value if hasattr(result, 'equipment') else None,
              confidence_score=result.overall_confidence,
              extraction_result={
                  "raw_data": result.model_dump(),
                  "confidence_scores": {},  # Will be populated by validation
                  "extraction_errors": result.extraction_errors,
              },
          )
          session.add(analysis)

          # 4. Update task
          task.status = TaskStatus.COMPLETED.value
          await session.commit()
          set_job_status(task_id, JobStatus.COMPLETED, result={"needs_review": result.needs_review})

      except Exception as e:
          task.status = TaskStatus.FAILED.value
          task.error_message = str(e)
          await session.commit()
          set_job_status(task_id, JobStatus.FAILED, error=str(e))
          raise  # Re-raise for Dramatiq retry

Update app/worker/__init__.py to import extraction module.
  </action>
  <verify>python3 -c "from app.worker.extraction import process_document_task"</verify>
  <done>Extraction worker with retry and status tracking</done>
</task>

<task type="auto">
  <name>Task 3: Create task status API endpoint</name>
  <files>app/api/tasks.py, app/main.py</files>
  <action>
Create API endpoint to check task status.

app/api/tasks.py:
- router = APIRouter(prefix="/api/tasks", tags=["tasks"])

class TaskStatusResponse(BaseModel):
  task_id: UUID
  status: TaskStatus
  original_filename: str
  file_size: int
  created_at: datetime
  error_message: str | None = None
  analysis_id: UUID | None = None
  needs_review: bool | None = None

GET /api/tasks/{task_id}:
- Get task by ID
- Return 404 if not found
- Include analysis_id if analysis exists
- Include needs_review from job status

GET /api/tasks/{task_id}/result:
- Get task with analysis result
- Return 404 if not found
- Return 202 if still processing
- Return full extraction result when complete

class TaskResultResponse(BaseModel):
  task_id: UUID
  status: TaskStatus
  analysis: Analysis | None = None  # Full analysis with extraction_result

Update app/main.py to include tasks router.
  </action>
  <verify>curl http://localhost:8000/api/tasks/{task_id} (after starting server)</verify>
  <done>Task status and result endpoints available</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] process_document handles PDF and image files
- [ ] Test type is auto-detected from content
- [ ] Worker processes tasks with retry on failure
- [ ] Status API returns current task state
- [ ] Analysis record is created with extraction result
- [ ] needs_review flag is set for low-confidence extractions
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- End-to-end pipeline: upload → queue → extract → store → query
- Multi-page PDF support working
</success_criteria>

<output>
After completion, create `.planning/phases/02-extraction-pipeline/02-06-SUMMARY.md`
</output>
