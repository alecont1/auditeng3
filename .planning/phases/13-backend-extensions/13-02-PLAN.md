---
phase: 13-backend-extensions
plan: 02
type: execute
wave: 2
depends_on: ["13-01"]
files_modified:
  - app/services/audit.py
  - app/api/schemas.py
  - app/api/analyses.py
  - app/db/models/analysis.py
autonomous: true

must_haves:
  truths:
    - "PUT /api/analyses/{id}/approve updates verdict to 'approved'"
    - "PUT /api/analyses/{id}/reject updates verdict to 'rejected' with reason"
    - "Approve/reject only allowed on completed analyses"
    - "Human review actions are logged to audit trail"
    - "Only analysis owner can approve/reject"
  artifacts:
    - path: "app/services/audit.py"
      provides: "HUMAN_REVIEW_APPROVED, HUMAN_REVIEW_REJECTED event types and logging"
      contains: "HUMAN_REVIEW_APPROVED"
    - path: "app/api/schemas.py"
      provides: "RejectRequest schema"
      contains: "class RejectRequest"
    - path: "app/api/analyses.py"
      provides: "approve_analysis, reject_analysis endpoints"
      contains: "async def approve_analysis"
    - path: "app/db/models/analysis.py"
      provides: "rejection_reason field"
      contains: "rejection_reason"
  key_links:
    - from: "approve_analysis endpoint"
      to: "AuditService.log_human_review"
      via: "service call after verdict update"
      pattern: "log_human_review.*approved"
    - from: "reject_analysis endpoint"
      to: "AuditService.log_human_review"
      via: "service call after verdict update"
      pattern: "log_human_review.*rejected"
---

<objective>
Implement PUT /api/analyses/{id}/approve and PUT /api/analyses/{id}/reject endpoints with audit logging.

Purpose: Frontend review workflow needs endpoints to approve or reject analyses after engineer review.
Output: Working approve/reject endpoints with audit trail entries.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/milestones/v2.0-ROADMAP.md

# Existing code to extend:
@app/services/audit.py
@app/api/schemas.py
@app/api/analyses.py
@app/db/models/analysis.py
@app/schemas/enums.py
@app/core/auth.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add rejection_reason field to Analysis model</name>
  <files>app/db/models/analysis.py</files>
  <action>
Add a new nullable field to the Analysis model to store rejection reasons:

```python
rejection_reason: Mapped[Optional[str]] = mapped_column(
    Text,
    nullable=True,
)
```

Add the import: `from sqlalchemy import Text` (if not already imported).

This field stores the reason when an analysis is rejected by human review.
  </action>
  <verify>python -c "from app.db.models import Analysis; print(hasattr(Analysis, 'rejection_reason'))"</verify>
  <done>Analysis model has rejection_reason field</done>
</task>

<task type="auto">
  <name>Task 2: Add human review event types to AuditService</name>
  <files>app/services/audit.py</files>
  <action>
1. Add new event types to the EventType StrEnum:
   - HUMAN_REVIEW_APPROVED = "human_review_approved"
   - HUMAN_REVIEW_REJECTED = "human_review_rejected"

2. Add a new static method to AuditService:

```python
@staticmethod
async def log_human_review(
    db: AsyncSession,
    analysis_id: uuid.UUID,
    action: str,  # "approved" or "rejected"
    user_id: uuid.UUID,
    reason: Optional[str] = None,
) -> AuditLog:
    """Log a human review action (approve/reject).

    Args:
        db: Database session.
        analysis_id: ID of the analysis being reviewed.
        action: "approved" or "rejected".
        user_id: ID of the user performing the review.
        reason: Optional rejection reason (required for rejections).

    Returns:
        Created AuditLog record.
    """
    event_type = (
        EventType.HUMAN_REVIEW_APPROVED
        if action == "approved"
        else EventType.HUMAN_REVIEW_REJECTED
    )

    details = {
        "action": action,
        "reviewer_id": str(user_id),
    }
    if reason:
        details["reason"] = reason

    log = AuditLog(
        id=uuid.uuid4(),
        analysis_id=analysis_id,
        event_type=event_type,
        event_timestamp=datetime.now(timezone.utc),
        details=details,
    )
    db.add(log)
    return log
```
  </action>
  <verify>python -c "from app.services.audit import EventType, AuditService; print(EventType.HUMAN_REVIEW_APPROVED); print(hasattr(AuditService, 'log_human_review'))"</verify>
  <done>EventType has HUMAN_REVIEW_APPROVED/REJECTED and AuditService has log_human_review method</done>
</task>

<task type="auto">
  <name>Task 3: Add approve/reject schemas and endpoints</name>
  <files>app/api/schemas.py, app/api/analyses.py</files>
  <action>
**In app/api/schemas.py**, add:

```python
class RejectRequest(BaseModel):
    """Request body for rejecting an analysis."""

    reason: str = Field(
        ...,
        min_length=10,
        max_length=1000,
        description="Reason for rejection (required, 10-1000 chars)"
    )

class ApproveRejectResponse(BaseModel):
    """Response for approve/reject actions."""

    analysis_id: UUID = Field(..., description="Analysis ID")
    verdict: str = Field(..., description="New verdict (approved/rejected)")
    message: str = Field(..., description="Success message")

    model_config = {"from_attributes": True}
```

**In app/api/analyses.py**, add two new endpoints:

1. **PUT /api/analyses/{analysis_id}/approve**:
```python
@router.put(
    "/{analysis_id}/approve",
    response_model=ApproveRejectResponse,
    summary="Approve analysis",
    description="Mark a completed analysis as approved. Requires ownership.",
    responses={
        200: {"description": "Analysis approved"},
        400: {"description": "Analysis not completed or already has verdict"},
        401: {"description": "Not authenticated"},
        403: {"description": "Access denied"},
        404: {"description": "Analysis not found"},
    },
)
async def approve_analysis(
    analysis_id: UUID,
    db: DbSession,
    current_user: CurrentUser,
) -> ApproveRejectResponse:
```

Implementation:
- Call verify_analysis_ownership() to check access
- Check task.status == "completed" (otherwise 400)
- Check analysis.verdict is None or "review" (can't re-approve/reject)
- Update analysis.verdict = "approved"
- Call AuditService.log_human_review(db, analysis_id, "approved", current_user.id)
- Commit and return success response

2. **PUT /api/analyses/{analysis_id}/reject**:
```python
@router.put(
    "/{analysis_id}/reject",
    response_model=ApproveRejectResponse,
    summary="Reject analysis",
    description="Mark a completed analysis as rejected with reason. Requires ownership.",
    responses={
        200: {"description": "Analysis rejected"},
        400: {"description": "Analysis not completed or already has verdict"},
        401: {"description": "Not authenticated"},
        403: {"description": "Access denied"},
        404: {"description": "Analysis not found"},
    },
)
async def reject_analysis(
    analysis_id: UUID,
    request: RejectRequest,
    db: DbSession,
    current_user: CurrentUser,
) -> ApproveRejectResponse:
```

Implementation:
- Call verify_analysis_ownership() to check access
- Check task.status == "completed" (otherwise 400)
- Check analysis.verdict is None or "review"
- Update analysis.verdict = "rejected"
- Update analysis.rejection_reason = request.reason
- Call AuditService.log_human_review(db, analysis_id, "rejected", current_user.id, request.reason)
- Commit and return success response

Add imports:
- `from .schemas import RejectRequest, ApproveRejectResponse`
- `from app.services.audit import AuditService`
  </action>
  <verify>
Start server and test:
1. curl -X PUT "http://localhost:8000/api/analyses/{id}/approve" -H "Authorization: Bearer $TOKEN"
2. curl -X PUT "http://localhost:8000/api/analyses/{id}/reject" -H "Authorization: Bearer $TOKEN" -H "Content-Type: application/json" -d '{"reason": "Measurement values out of acceptable range"}'
  </verify>
  <done>
- PUT /api/analyses/{id}/approve returns 200 and updates verdict
- PUT /api/analyses/{id}/reject returns 200 with reason stored
- Both endpoints create audit log entries
- Proper error handling for incomplete analyses
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -c "from app.db.models import Analysis; print(hasattr(Analysis, 'rejection_reason'))"` prints True
- [ ] `python -c "from app.services.audit import EventType; print(EventType.HUMAN_REVIEW_APPROVED)"` succeeds
- [ ] `python -c "from app.api.analyses import approve_analysis, reject_analysis"` succeeds
- [ ] Server starts without errors: `uvicorn app.main:app --reload`
- [ ] Endpoints documented in OpenAPI: visit /docs and check approve/reject
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- No Python import errors
- Approve endpoint updates verdict and creates audit log
- Reject endpoint stores reason and creates audit log
- Security: only analysis owner can approve/reject
- Validation: only completed analyses can be reviewed
</success_criteria>

<output>
After completion, create `.planning/phases/13-backend-extensions/13-02-SUMMARY.md`
</output>
