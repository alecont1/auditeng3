# Plan 15-01: Cloudflare R2 Storage Integration

## Frontmatter

```yaml
phase: 15
plan: 01
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - app/config.py
  - app/services/storage.py
  - app/api/analyses.py
  - app/api/upload.py
  - app/worker/extraction.py
autonomous: true
estimated_tasks: 6
```

## Goal

Replace local filesystem storage with Cloudflare R2 so worker containers can access uploaded files from backend-api containers.

## Context

**Problem:** Railway runs backend-api and worker in separate containers. Backend saves files to local `./uploads/`, but worker can't access that filesystem.

**Solution:** Use Cloudflare R2 (S3-compatible) as shared storage. Both services upload/download via boto3.

## Prerequisites

User must create in Cloudflare Dashboard:
1. R2 bucket (e.g., `auditeng-uploads`)
2. R2 API Token with read/write permissions
3. Note Account ID from dashboard

## Tasks

<task id="1">
<title>Add boto3 dependency</title>
<type>code</type>
<action>
Add boto3 to pyproject.toml dependencies:
```python
"boto3>=1.35.0",
```
</action>
<verification>Dependency added to pyproject.toml</verification>
</task>

<task id="2">
<title>Add R2 configuration to Settings</title>
<type>code</type>
<action>
Edit `app/config.py` to add R2 settings:

```python
# Cloudflare R2 configuration
R2_ACCOUNT_ID: str = ""
R2_ACCESS_KEY_ID: str = ""
R2_SECRET_ACCESS_KEY: str = ""
R2_BUCKET_NAME: str = "auditeng-uploads"
```

These will be set via environment variables in Railway.
</action>
<verification>Settings class has R2 fields</verification>
</task>

<task id="3">
<title>Refactor storage.py for R2</title>
<type>code</type>
<action>
Rewrite `app/services/storage.py` to use R2:

```python
"""File storage service using Cloudflare R2.

Files are stored in R2 bucket organized by task_id.
S3-compatible API via boto3.
"""

import io
import logging
from uuid import UUID

import boto3
from botocore.config import Config
from fastapi import UploadFile

from app.config import get_settings

logger = logging.getLogger(__name__)
settings = get_settings()

# Initialize R2 client (S3-compatible)
def get_r2_client():
    """Get configured R2 client."""
    return boto3.client(
        service_name="s3",
        endpoint_url=f"https://{settings.R2_ACCOUNT_ID}.r2.cloudflarestorage.com",
        aws_access_key_id=settings.R2_ACCESS_KEY_ID,
        aws_secret_access_key=settings.R2_SECRET_ACCESS_KEY,
        config=Config(signature_version="s3v4"),
        region_name="auto",
    )


def get_object_key(task_id: UUID, filename: str) -> str:
    """Get the R2 object key for a file.

    Args:
        task_id: The unique identifier for the task.
        filename: The name of the file.

    Returns:
        str: The object key (path) in R2.
    """
    return f"{task_id}/{filename}"


async def save_file(task_id: UUID, file: UploadFile) -> tuple[str, int]:
    """Save an uploaded file to R2.

    Args:
        task_id: The unique identifier for the task.
        file: The uploaded file from FastAPI.

    Returns:
        tuple[str, int]: The object key and file size in bytes.
    """
    filename = file.filename or "document"
    object_key = get_object_key(task_id, filename)

    # Read file content
    content = await file.read()
    file_size = len(content)

    # Upload to R2
    client = get_r2_client()
    client.upload_fileobj(
        io.BytesIO(content),
        settings.R2_BUCKET_NAME,
        object_key,
        ExtraArgs={"ContentType": file.content_type or "application/octet-stream"}
    )

    logger.info(f"Uploaded {object_key} to R2 ({file_size} bytes)")
    return object_key, file_size


async def get_file(task_id: UUID, filename: str) -> bytes | None:
    """Download a file from R2.

    Args:
        task_id: The unique identifier for the task.
        filename: The name of the file.

    Returns:
        bytes | None: The file content if exists, None otherwise.
    """
    object_key = get_object_key(task_id, filename)
    client = get_r2_client()

    try:
        response = client.get_object(
            Bucket=settings.R2_BUCKET_NAME,
            Key=object_key
        )
        return response["Body"].read()
    except client.exceptions.NoSuchKey:
        logger.warning(f"File not found in R2: {object_key}")
        return None
    except Exception as e:
        logger.error(f"Error downloading from R2: {e}")
        return None


def delete_task_files(task_id: UUID) -> None:
    """Delete all files for a task from R2.

    Args:
        task_id: The unique identifier for the task.
    """
    client = get_r2_client()
    prefix = f"{task_id}/"

    try:
        # List objects with prefix
        response = client.list_objects_v2(
            Bucket=settings.R2_BUCKET_NAME,
            Prefix=prefix
        )

        if "Contents" in response:
            objects = [{"Key": obj["Key"]} for obj in response["Contents"]]
            if objects:
                client.delete_objects(
                    Bucket=settings.R2_BUCKET_NAME,
                    Delete={"Objects": objects}
                )
                logger.info(f"Deleted {len(objects)} files for task {task_id}")
    except Exception as e:
        logger.error(f"Error deleting files from R2: {e}")
```
</action>
<verification>storage.py uses boto3 with R2</verification>
</task>

<task id="4">
<title>Update upload API to store object key</title>
<type>code</type>
<action>
In `app/api/upload.py` and `app/api/analyses.py`, the `save_file` function now returns `(object_key, file_size)` instead of `(file_path, file_size)`.

The `task.file_path` will now store the R2 object key (e.g., `uuid/filename.pdf`) instead of local path.

No change needed in API files since they already store the return value in `task.file_path`.
</action>
<verification>Task stores R2 object key in file_path</verification>
</task>

<task id="5">
<title>Update worker to download from R2</title>
<type>code</type>
<action>
Edit `app/worker/extraction.py` to download file from R2 before processing:

Replace lines 88-91:
```python
        try:
            # 3. Run extraction
            file_path = Path(task.file_path)
            result = await process_document(task_uuid, file_path)
```

With:
```python
        try:
            # 3. Download file from R2
            from app.services.storage import get_file
            import tempfile
            import os

            # Parse object key to get filename
            object_key = task.file_path  # Now stores R2 object key
            filename = object_key.split("/")[-1] if "/" in object_key else object_key

            file_content = await get_file(task_uuid, filename)
            if not file_content:
                raise FileNotFoundError(f"File not found in R2: {object_key}")

            # Write to temp file for extraction
            with tempfile.NamedTemporaryFile(delete=False, suffix=f"_{filename}") as tmp:
                tmp.write(file_content)
                temp_path = tmp.name

            try:
                # 4. Run extraction
                file_path = Path(temp_path)
                result = await process_document(task_uuid, file_path)
            finally:
                # Cleanup temp file
                if os.path.exists(temp_path):
                    os.unlink(temp_path)
```

Also update the import at top:
```python
import os
import tempfile
```
</action>
<verification>Worker downloads from R2 before extraction</verification>
</task>

<task id="6">
<title>Set Railway environment variables</title>
<type>manual</type>
<action>
In Railway dashboard, add to BOTH backend-api AND worker services:

```
R2_ACCOUNT_ID=<your-cloudflare-account-id>
R2_ACCESS_KEY_ID=<your-r2-api-token-access-key>
R2_SECRET_ACCESS_KEY=<your-r2-api-token-secret>
R2_BUCKET_NAME=auditeng-uploads
```
</action>
<verification>Environment variables set in Railway</verification>
</task>

## Verification Criteria

- [ ] boto3 in dependencies
- [ ] R2 settings in config.py
- [ ] storage.py uses R2 via boto3
- [ ] Worker downloads file from R2 before extraction
- [ ] Task.file_path stores R2 object key
- [ ] Environment variables set in Railway

## must_haves

- Worker can access files uploaded by backend-api
- No local filesystem dependency
- Existing API contracts unchanged
- Cleanup deletes from R2

## Rollback

If issues occur:
1. Revert storage.py to filesystem version
2. Remove boto3 from dependencies
3. Remove R2 settings from config
4. Redeploy
