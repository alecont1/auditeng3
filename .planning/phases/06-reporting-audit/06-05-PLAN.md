---
phase: 06-reporting-audit
plan: 05
type: execute
wave: 1
depends_on: []
files_modified:
  - app/core/validation/schemas.py
  - app/core/validation/base.py
  - app/worker/extraction.py
  - app/tests/services/test_audit.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "All validation rule evaluations are logged with rule IDs"
    - "Passing rules are logged as well as failing rules"
    - "Audit trail shows complete rule evaluation history"
  artifacts:
    - path: "app/core/validation/schemas.py"
      provides: "RuleEvaluation schema for tracking rule checks"
      contains: "class RuleEvaluation"
    - path: "app/core/validation/base.py"
      provides: "track_rule() helper method"
      contains: "def track_rule"
  key_links:
    - from: "app/worker/extraction.py"
      to: "AuditService.log_validation_rule"
      via: "loop over rules_evaluated"
      pattern: "log_validation_rule"
---

<objective>
Close gap #7: Integrate validation rule logging into extraction worker.

Purpose: The infrastructure for logging validation rules exists (`AuditService.log_validation_rule()`) but is never called. This plan integrates rule evaluation tracking so ALL rules (passed and failed) are logged for compliance auditing.

Output: ValidationResult includes `rules_evaluated` list, validators track all rule checks, worker logs each to AuditLog.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-reporting-audit/06-VERIFICATION.md

# Existing code to modify
@app/core/validation/schemas.py
@app/core/validation/base.py
@app/worker/extraction.py
@app/services/audit.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add RuleEvaluation schema and rules_evaluated to ValidationResult</name>
  <files>app/core/validation/schemas.py</files>
  <action>
  1. Add new `RuleEvaluation` schema to track each rule check:
     - rule_id: str
     - passed: bool
     - details: dict | None (optional context like threshold, extracted_value)

  2. Add `rules_evaluated: list[RuleEvaluation] = Field(default_factory=list)` to ValidationResult

  This enables validators to track every rule check (passed or failed), not just failures.
  </action>
  <verify>python -c "from app.core.validation.schemas import RuleEvaluation, ValidationResult; r = ValidationResult(test_type='test'); print('rules_evaluated' in r.model_fields)"</verify>
  <done>RuleEvaluation schema exists, ValidationResult has rules_evaluated field</done>
</task>

<task type="auto">
  <name>Task 2: Add track_rule helper to BaseValidator</name>
  <files>app/core/validation/base.py</files>
  <action>
  1. Import RuleEvaluation from schemas

  2. Add `track_rule()` method to BaseValidator:
     ```python
     def track_rule(
         self,
         rules: list[RuleEvaluation],
         rule_id: str,
         passed: bool,
         details: dict | None = None,
     ) -> None:
         """Track a rule evaluation (passed or failed).

         Args:
             rules: List to append the evaluation to.
             rule_id: Unique rule identifier.
             passed: Whether the rule passed.
             details: Optional context (threshold, extracted_value).
         """
         rules.append(RuleEvaluation(
             rule_id=rule_id,
             passed=passed,
             details=details,
         ))
     ```

  3. Modify `add_finding()` to also call `track_rule()` with passed=False

  4. Modify `create_result()` to accept and include rules_evaluated

  Validators will call track_rule() for passing checks and add_finding() for failures.
  </action>
  <verify>python -c "from app.core.validation.base import BaseValidator; print(hasattr(BaseValidator, 'track_rule'))"</verify>
  <done>BaseValidator has track_rule method, add_finding also tracks the rule as failed</done>
</task>

<task type="auto">
  <name>Task 3: Integrate rule logging in extraction worker</name>
  <files>app/worker/extraction.py</files>
  <action>
  1. After `validation_result = ValidationOrchestrator().validate(extraction)`:
     - Loop through `validation_result.rules_evaluated`
     - For each rule evaluation, call:
       ```python
       await AuditService.log_validation_rule(
           db=db,
           analysis_id=analysis.id,
           rule_id=rule_eval.rule_id,
           passed=rule_eval.passed,
           details=rule_eval.details,
       )
       ```

  2. Wrap in try/except like other audit calls (fail open - don't break extraction on audit failure)

  This completes AUDT-02: All validation decisions are logged with rule IDs.
  </action>
  <verify>grep -n "log_validation_rule" app/worker/extraction.py | head -5</verify>
  <done>Worker calls log_validation_rule for each rule evaluation</done>
</task>

<task type="auto">
  <name>Task 4: Add integration test for validation rule logging</name>
  <files>app/tests/services/test_audit.py</files>
  <action>
  Add test that verifies validation rules are logged during extraction:

  1. Create test `test_validation_rules_logged_during_extraction`:
     - Set up analysis with known validation rules
     - Run extraction worker (or simulate validation)
     - Query AuditLog for VALIDATION_RULE_APPLIED events
     - Assert both passed and failed rules are logged
     - Assert rule_id is captured

  This ensures the integration works end-to-end.
  </action>
  <verify>python -m pytest app/tests/services/test_audit.py::test_validation_rules_logged_during_extraction -v 2>/dev/null || python -m pytest app/tests/services/test_audit.py -v -k "validation_rule"</verify>
  <done>Test exists and passes, verifying rule logging integration</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -c "from app.core.validation.schemas import RuleEvaluation"` succeeds
- [ ] `python -c "from app.core.validation.base import BaseValidator; print(hasattr(BaseValidator, 'track_rule'))"` prints True
- [ ] `grep "log_validation_rule" app/worker/extraction.py` finds call in worker
- [ ] `python -m pytest app/tests/services/test_audit.py -v` all tests pass
</verification>

<success_criteria>
- RuleEvaluation schema added to validation schemas
- ValidationResult includes rules_evaluated list
- BaseValidator has track_rule() method
- Extraction worker logs all rule evaluations via AuditService
- Test verifies end-to-end rule logging
- All existing tests still pass
</success_criteria>

<output>
After completion, create `.planning/phases/06-reporting-audit/06-05-SUMMARY.md`
</output>
