---
phase: 05-api-findings
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - app/services/finding.py
  - app/services/verdict.py
  - app/services/__init__.py
autonomous: true

must_haves:
  truths:
    - "Findings are generated from validation results with evidence"
    - "Compliance score is calculated (100 - penalties)"
    - "Verdict is APPROVED when no CRITICAL and score >= 95"
    - "Verdict is REVIEW when score 80-94 or low confidence"
    - "Verdict is REJECTED when any CRITICAL finding exists"
  artifacts:
    - path: "app/services/finding.py"
      provides: "Finding generation from validation"
      exports: ["FindingService", "generate_findings_from_validation"]
    - path: "app/services/verdict.py"
      provides: "Scoring and verdict computation"
      exports: ["VerdictService", "compute_verdict", "compute_compliance_score"]
  key_links:
    - from: "app/services/finding.py"
      to: "app/core/validation/schemas.py"
      via: "Finding conversion"
      pattern: "ValidationResult|Finding"
    - from: "app/services/verdict.py"
      to: "app/schemas/enums.py"
      via: "Verdict enum usage"
      pattern: "Verdict\\."
---

<objective>
Implement finding generation service and verdict computation logic.

Purpose: Generate findings with evidence and compute compliance scores/verdicts per FIND-01 through FIND-09 requirements.
Output: FindingService for finding generation, VerdictService for scoring and verdicts.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Validation engine provides ValidationResult and Finding schemas
@.planning/phases/03-validation-engine/03-01-SUMMARY.md

# Existing validation orchestrator has calculate_compliance_score
@app/core/validation/orchestrator.py
@app/core/validation/schemas.py

# Finding ORM model for persistence
@app/db/models/finding.py
@app/db/models/analysis.py

# Enums include Verdict and FindingSeverity
@app/schemas/enums.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create finding generation service</name>
  <files>app/services/finding.py</files>
  <action>
    Create app/services/finding.py with FindingService class:

    1. Convert validation Finding to database Finding:
       - generate_finding_from_validation(
           validation_finding: ValidationFinding,
           analysis_id: UUID
         ) -> FindingCreate
       - Map fields: rule_id, severity, message
       - Build evidence dict: {extracted_value, threshold, standard_reference}
       - Add remediation from validation finding

    2. Batch generation:
       - generate_findings_from_validation(
           validation_result: ValidationResult,
           analysis_id: UUID
         ) -> list[FindingCreate]
       - Convert all findings from ValidationResult
       - Return list of FindingCreate schemas

    3. Persist findings:
       - persist_findings(
           db: AsyncSession,
           findings: list[FindingCreate]
         ) -> list[Finding]
       - Bulk insert to database
       - Return persisted Finding ORM objects

    Use existing ValidationResult and Finding from app.core.validation.schemas.
    Use FindingCreate from app.schemas.finding.
  </action>
  <verify>python -c "from app.services.finding import FindingService; print('OK')"</verify>
  <done>FindingService converts validation findings to database findings</done>
</task>

<task type="auto">
  <name>Task 2: Create verdict service with scoring logic</name>
  <files>app/services/verdict.py</files>
  <action>
    Create app/services/verdict.py with VerdictService class:

    1. Compliance score calculation (per existing orchestrator logic):
       - compute_compliance_score(validation_result: ValidationResult) -> float
       - Start at 100.0
       - Subtract: CRITICAL * 25, MAJOR * 10, MINOR * 2
       - Clamp to 0-100 range
       - Return rounded to 2 decimal places

    2. Confidence score calculation:
       - compute_confidence_score(extraction_result: BaseExtractionResult) -> float
       - Calculate average confidence from all FieldConfidence values
       - Use metadata.overall_confidence if available
       - Return 0.0-1.0 value

    3. Verdict determination (per requirements):
       - compute_verdict(
           validation_result: ValidationResult,
           compliance_score: float,
           confidence_score: float
         ) -> Verdict
       - REJECTED: Any CRITICAL finding (validation_result.critical_count > 0)
       - REVIEW: compliance_score < 95 OR confidence_score < 0.7
       - APPROVED: No CRITICAL, score >= 95, confidence >= 0.7

    4. Complete analysis verdict:
       - compute_analysis_verdict(
           validation_result: ValidationResult,
           extraction_result: BaseExtractionResult
         ) -> tuple[Verdict, float, float]
       - Combines all above into single call
       - Returns (verdict, compliance_score, confidence_score)

    Use Verdict enum from app.schemas.enums (APPROVED, REVIEW, REJECTED).
  </action>
  <verify>python -c "from app.services.verdict import VerdictService, compute_verdict; print('OK')"</verify>
  <done>VerdictService computes scores and verdicts per requirements</done>
</task>

<task type="auto">
  <name>Task 3: Add unit tests for finding and verdict services</name>
  <files>app/tests/services/__init__.py, app/tests/services/test_finding.py, app/tests/services/test_verdict.py</files>
  <action>
    Create test files with pytest:

    1. app/tests/services/__init__.py - empty package file

    2. app/tests/services/test_finding.py:
       - test_generate_finding_preserves_fields: Check all fields mapped
       - test_generate_findings_batch: Multiple findings from ValidationResult
       - test_evidence_structure: Evidence dict has correct keys

    3. app/tests/services/test_verdict.py:
       - test_compliance_score_perfect: No findings = 100.0
       - test_compliance_score_with_findings: Correct deductions
       - test_compliance_score_clamped_to_zero: Many findings = 0.0
       - test_verdict_rejected_on_critical: CRITICAL → REJECTED
       - test_verdict_review_low_score: Score < 95 → REVIEW
       - test_verdict_review_low_confidence: Confidence < 0.7 → REVIEW
       - test_verdict_approved: No CRITICAL, score >= 95, confidence >= 0.7 → APPROVED

    Use pytest fixtures to create sample ValidationResult and extraction data.
    Mock database session for finding persistence tests.
  </action>
  <verify>pytest app/tests/services/ -v</verify>
  <done>All verdict and finding service tests pass</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -c "from app.services.finding import FindingService"` succeeds
- [ ] `python -c "from app.services.verdict import VerdictService"` succeeds
- [ ] pytest app/tests/services/ passes all tests
- [ ] Compliance score calculation matches: 100 - (CRITICAL*25 + MAJOR*10 + MINOR*2)
- [ ] Verdict REJECTED when any CRITICAL
- [ ] Verdict REVIEW when score < 95 or confidence < 0.7
- [ ] Verdict APPROVED when no CRITICAL, score >= 95, confidence >= 0.7
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- No errors or warnings introduced
- Finding generation converts validation findings to database schema
- Verdict logic matches FIND-05, FIND-06, FIND-07 requirements exactly
</success_criteria>

<output>
After completion, create `.planning/phases/05-api-findings/05-02-SUMMARY.md`
</output>
